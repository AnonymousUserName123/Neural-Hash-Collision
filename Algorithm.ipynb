{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce44f706-4f3c-4744-ae58-8eeb76b7d4f5",
   "metadata": {},
   "source": [
    "Import modules and check that version is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02491d2-52e3-4549-a149-87f6ea51501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime 1.18.1\n",
      "onnx 1.17.0\n",
      "simplifier 0.4.33\n",
      "surgeon 0.5.6\n",
      "processing tools 1.1.32\n",
      "tensorflow 2.17.0\n",
      "tfkeras 2.19.0\n",
      "ai 1.2.0\n",
      "psutil 5.9.5\n",
      "ml_dtypes 0.5.1\n",
      "flat 24.3.25\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import onnxruntime as ort\n",
    "print(\"runtime\", ort.__version__)\n",
    "import onnx\n",
    "print(\"onnx\", onnx.__version__)\n",
    "import onnxsim \n",
    "print(\"simplifier\", onnxsim.__version__)\n",
    "import onnx_graphsurgeon\n",
    "print(\"surgeon\", onnx_graphsurgeon.__version__)\n",
    "import simple_onnx_processing_tools\n",
    "print(\"processing tools\",simple_onnx_processing_tools.__version__)\n",
    "import tensorflow as tf\n",
    "print(\"tensorflow\",tf.__version__)\n",
    "import tf_keras\n",
    "print(\"tfkeras\",tf_keras.__version__)\n",
    "import ai_edge_litert\n",
    "print(\"ai\", ai_edge_litert.__version__)\n",
    "import psutil\n",
    "print(\"psutil\", psutil.__version__)\n",
    "import ml_dtypes\n",
    "print(\"ml_dtypes\", ml_dtypes.__version__)\n",
    "import flatbuffers\n",
    "print(\"flat\", flatbuffers.__version__)\n",
    "import onnx2tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad58f0-12e0-4af0-9009-201585a32e4b",
   "metadata": {},
   "source": [
    "Import onnx model and convert it into tensorflow using the onnx2tf module. To acquire the onnx NeuralHash model (the that we convert below) follows the steps in https://github.com/AsuharietYgvar/AppleNeuralHash2ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebafa86d-869b-4c36-aba7-ab87d722f4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path=\"/Users/omaralmutoteh/NeuralHash/model.onnx\"\n",
    "tf_model=onnx2tf.convert(input_onnx_file_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f56cb8-f4b9-4050-ad01-ff4bad20a87a",
   "metadata": {},
   "source": [
    "Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5eeaebb-afce-43ab-a552-1e000cc710b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def load_image_tf(path):\n",
    "    im = Image.open(path).convert('RGB')\n",
    "    im = im.resize([360, 360])\n",
    "    arr = np.array(im).astype(np.float32) / 255.0\n",
    "    arr = arr.reshape([1, 360, 360, 3])\n",
    "    return arr\n",
    "\n",
    "def save_image(arr, path):\n",
    "    arr = arr.reshape([360, 360, 3]) # changes shape of array from (1, 360, 360, 3) to (360, 360, 3)\n",
    "    arr = (arr) * (255.0)\n",
    "    arr = arr.astype(np.uint8)\n",
    "    im = Image.fromarray(arr)\n",
    "    im.save(path)\n",
    "\n",
    "def load_seed(path):\n",
    "    seed = open(path, 'rb').read()[128:]\n",
    "    seed = np.frombuffer(seed, dtype=np.float32)\n",
    "    seed = seed.reshape([96, 128])\n",
    "    return seed\n",
    "\n",
    "def hash_from_hex(hex_repr):\n",
    "    n = int(hex_repr, 16)\n",
    "    h = np.zeros(96)\n",
    "    for i in range(96):\n",
    "        h[i] = (n >> (95 - i)) & 1\n",
    "    return h\n",
    "\n",
    "def hash_to_hex(h):\n",
    "    bits = ''.join(['1' if i >= 0.5 else '0' for i in h])\n",
    "    return '{:0{}x}'.format(int(bits, 2), len(bits) // 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c46c94-7144-4dc0-b6e2-cb7233a3f11a",
   "metadata": {},
   "source": [
    "In the cell below we go thru the method by which our tf2 keras model is converted into its serialzed form so that it can be deployed in a tf1 graph session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab47e13-094f-45f8-bda9-e391c41992ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744546182.665960 6599105 devices.cc:76] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\"\"\"\n",
    "Our main goal is to run a tf1 session which executes the graph version of \n",
    "tf_model.\n",
    "Because our model is a tf2 keras model what we're demanding is a tough ask,  \n",
    "since one can only convert tf_model into a tf2 graph which isn't executable\n",
    "on tf1 sessions (due to tf2's management of a graph's content as variables,\n",
    "which tf1 has no idea how to handle)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The first step is to just convert your model into a tf2 graph, or what is \n",
    "known as a static graph and this is done by tracing the model \n",
    "thru this decorator business\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "\n",
    "\"\"\"\n",
    "In the lines below we use the @ decorator to convert our tf_model func\n",
    "into its graph equivalent. \n",
    "This only produces a template graph, to generate the version which u can execute on\n",
    "we need to do the .get_concrete_function() call.\n",
    "As it stands we now have our tf2 graph, which is a fine object. You will\n",
    "be able to eagerly execute your model on inputs, and your tf2 graph will\n",
    "do what its supposed to and execute in a graph way.\n",
    "The problem is that you can't, as it stands, execute this graph in a tf1 \n",
    "session.\n",
    "\"\"\"\n",
    "\n",
    "model = tf_model\n",
    "@tf.function(input_signature=[tf.TensorSpec([1, 360, 360, 3], tf.float32)])\n",
    "def model_func(x):\n",
    "    return model(x)\n",
    "concrete_func = model_func.get_concrete_function()\n",
    "\n",
    "\"\"\"\n",
    "To convert our graph so that we can run a tf1 sess on it,\n",
    "we will produce a blank tf1 graph, work within it and then \n",
    "populate it with our tf2 graph's values.\n",
    "To populate a blank graph with another graph we need serialize \n",
    "the latter graph using .graph.as_graph_def and then load it in\n",
    "a tf1 session using tf.import_graph_def to get \n",
    "the unseralized version of the graph (i.e. our desired graph).\n",
    "The convert_variables_to_constants_v2 call converts the \n",
    "values in your graph from type variables to type constants. \n",
    "We need to do this so that we get our graph to the point\n",
    "whereby its eligible to be converted into its pb format (serialzed form) and \n",
    "so it's exportable to tf1.\n",
    "\"\"\"\n",
    "\n",
    "frozen_func = convert_variables_to_constants_v2(concrete_func)\n",
    "\n",
    "\"\"\"\n",
    "The below converts our graph into pb format.\n",
    "\"\"\"\n",
    "\n",
    "graph_def = frozen_func.graph.as_graph_def()\n",
    "\n",
    "\"\"\"\n",
    "Creates our blank graph.\n",
    "\"\"\"\n",
    "\n",
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb9388e-243c-48f2-afc8-2a27cc13db53",
   "metadata": {},
   "source": [
    "In the cell below we use the seralized form of our tf model to build its equivalent tf1 graph. Moreover we extend the graph so that upon running a session on it we are able to achieve values which are required for our ultimate on the cat image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6dd4fa4c-217f-46d9-90e3-4927909c8249",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The with loop below allows us to work with our \n",
    "blank tf1 graph.\n",
    "\"\"\"\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    \"\"\"\n",
    "    We will build our desired graph by populating \n",
    "    our blank graph with our tf2 graph using the pb version of the tf2 graph.\n",
    "    This is done thru the tf.import_graph_def call below.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.import_graph_def(graph_def, name=\"\")\n",
    "\n",
    "    \"\"\"\n",
    "    Now that we have the tf1 graph equiavlent of our model, we refer to \n",
    "    the input and output of our model by the name of the \"nodes\" in \n",
    "    the tf1 graph which correspond to those objects.\n",
    "    Moreover, when we define a new variable in terms of an old one (such \n",
    "    as here: logits = tf.reshape(logits_raw, (128,))) we are effectively \n",
    "    creating a new node within the graph which we can then recover \n",
    "    upon the execution of the graph (in the same way that we do to \n",
    "    get, say, the output node.\n",
    "    Finally, we can also create new nodes which don't have to be in terms of old \n",
    "    nodes thru this \"placeholder\" method. We want to do this so that we can \n",
    "    input in fixed values when we run the session, and this is done by \n",
    "    specifying the placeholder value.\n",
    "    Why are we creating new nodes?\n",
    "    Our attack on the original image is a gradient based one, thus we need to \n",
    "    be able to produce the gradients to alter the image in the correct direction.\n",
    "    Thus, gradients become a new node in our graph whose values can now be attained.\n",
    "    \"\"\"\n",
    "    \n",
    "    # From frozen graph\n",
    "    image_ph = graph.get_tensor_by_name(\"x:0\")              # input name is \"x:0\"\n",
    "    logits_raw = graph.get_tensor_by_name(\"Identity:0\")     # output name is \"Identity:0\"\n",
    "    logits = tf.reshape(logits_raw, (128,))                # need to reshape because of the unnecessary dims\n",
    "\n",
    "    # Our placeholders\n",
    "    original_ph = tf.compat.v1.placeholder(tf.float32, shape=(1, 360, 360, 3), name=\"original\")\n",
    "    seed_ph = tf.compat.v1.placeholder(tf.float32, shape=(96, 128), name=\"seed\")\n",
    "    h_ph = tf.compat.v1.placeholder(tf.float32, shape=(96,), name=\"h\")\n",
    "\n",
    "\n",
    "    # Hashing pipeline\n",
    "    proj = tf.reshape(tf.matmul(seed_ph, tf.reshape(logits, (128, 1))), (96,))\n",
    "\n",
    "    \"\"\"\n",
    "    We want to L2 normalize the output 96D vector because the following operation is \n",
    "    the sigmoid function which is 0,1 for large negative and positive numbers, respectively.\n",
    "    This means that without normalize, the hash_output node is effectively a 0,1 constant \n",
    "    function about negative and non-negative values. \n",
    "    Thus, its gradients will be 0.\n",
    "    Hence, we need to normalize the valyes of proj prior to entering them into sigmoid,\n",
    "    so that we don't get this 0,1 effect.\n",
    "    \"\"\"\n",
    "    \n",
    "    normalized, _ = tf.linalg.normalize(proj)\n",
    "\n",
    "    \"\"\"\n",
    "    We need to use the sigmoid func in place of \n",
    "    the neg, non-neg thresholding (which is normally used to achieve hash).\n",
    "    This is because the latter is not differentiable.\n",
    "    \"\"\"\n",
    "    \n",
    "    hash_output = tf.sigmoid(normalized)\n",
    "\n",
    "    # Losses\n",
    "    hash_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(h_ph, hash_output))\n",
    "\n",
    "    perturbation = image_ph - original_ph\n",
    "    img_loss = 0.99* tf.nn.l2_loss(perturbation) + 0.01* tf.image.total_variation(perturbation)[0]\n",
    "\n",
    "    combined_loss = 0.8*hash_loss + 0.2*img_loss\n",
    "\n",
    "    \n",
    "    # Gradients\n",
    "\n",
    "    \"\"\"\n",
    "    When calculating our desired gradients we ought to differentiate the losses wrt the input,\n",
    "    i.e. the image. \n",
    "    This is because we want to change our image until its respective loss is min.\n",
    "    \"\"\"\n",
    "    \n",
    "    g_hash = tf.gradients(hash_loss, image_ph)[0]\n",
    "    g_img, = tf.gradients(img_loss, image_ph)\n",
    "    g_combined, = tf.gradients(combined_loss, image_ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509ee7a-4f5e-4592-b59d-5a3ca5ed9ac3",
   "metadata": {},
   "source": [
    "In the cell below we go about loading the input values that we will use in when we run a session on our graph. For example it's here that we specify what the target hash's bits are, so that we can input it in the feed dictionary for the key h_ph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7780d6e-bef9-4d0f-af97-e0a9fb3c5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load inputs ===\n",
    "path_to_cat = \"/Users/omaralmutoteh/Desktop/GitHub/neural-hash-collider/cat.png\"\n",
    "seed_path = \"/Users/omaralmutoteh/NeuralHash/neuralhash_128x96_seed1.dat\"\n",
    "original_np = load_image_tf(path_to_cat)          # shape (1, 360, 360, 3)\n",
    "seed_np = load_seed(seed_path)           # shape (96, 128)\n",
    "h_np = hash_from_hex('59a34eabe31910abfb06f308')   # target hash's bits\n",
    "x = np.copy(original_np)\n",
    "best = 100  # (distance, image quality loss)\n",
    "dist = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c183e5fd-4317-4748-80b5-3b854bfbb9d8",
   "metadata": {},
   "source": [
    "It's in the cell below where we ultimately run our attack on the cat image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "462b9737-92bf-47da-ad2e-4d9201f10a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, best: 50, dist: 50, loss: hash\n",
      "iteration: 10, best: 40, dist: 40, loss: hash\n",
      "iteration: 20, best: 39, dist: 39, loss: hash\n",
      "iteration: 30, best: 35, dist: 35, loss: hash\n",
      "iteration: 40, best: 28, dist: 28, loss: hash\n",
      "iteration: 50, best: 25, dist: 26, loss: hash\n",
      "iteration: 60, best: 24, dist: 24, loss: hash\n",
      "iteration: 70, best: 22, dist: 24, loss: hash\n",
      "iteration: 80, best: 21, dist: 22, loss: hash\n",
      "iteration: 90, best: 19, dist: 19, loss: hash\n",
      "iteration: 100, best: 17, dist: 20, loss: hash\n",
      "iteration: 110, best: 17, dist: 17, loss: hash\n",
      "iteration: 120, best: 15, dist: 15, loss: hash\n",
      "iteration: 130, best: 13, dist: 14, loss: hash\n",
      "iteration: 140, best: 12, dist: 12, loss: hash\n",
      "iteration: 150, best: 12, dist: 13, loss: hash\n",
      "iteration: 160, best: 12, dist: 13, loss: hash\n",
      "iteration: 170, best: 12, dist: 13, loss: hash\n",
      "iteration: 180, best: 12, dist: 13, loss: hash\n",
      "iteration: 190, best: 12, dist: 12, loss: hash\n",
      "iteration: 200, best: 11, dist: 11, loss: hash\n",
      "iteration: 210, best: 11, dist: 11, loss: hash\n",
      "iteration: 220, best: 11, dist: 11, loss: hash\n",
      "iteration: 230, best: 11, dist: 11, loss: hash\n",
      "iteration: 240, best: 10, dist: 11, loss: hash\n",
      "iteration: 250, best: 9, dist: 10, loss: hash\n",
      "iteration: 260, best: 8, dist: 9, loss: hash\n",
      "iteration: 270, best: 6, dist: 6, loss: hash\n",
      "iteration: 280, best: 4, dist: 4, loss: hash\n",
      "iteration: 290, best: 3, dist: 3, loss: hash\n",
      "iteration: 300, best: 3, dist: 3, loss: hash\n",
      "iteration: 310, best: 3, dist: 3, loss: hash\n",
      "iteration: 320, best: 2, dist: 2, loss: hash\n",
      "iteration: 330, best: 1, dist: 3, loss: hash\n",
      "iteration: 340, best: 1, dist: 3, loss: hash\n",
      "iteration: 350, best: 1, dist: 2, loss: hash\n",
      "iteration: 360, best: 1, dist: 2, loss: hash\n",
      "iteration: 370, best: 1, dist: 2, loss: hash\n",
      "iteration: 380, best: 1, dist: 2, loss: hash\n",
      "iteration: 390, best: 1, dist: 2, loss: hash\n",
      "iteration: 400, best: 1, dist: 2, loss: hash\n",
      "iteration: 410, best: 1, dist: 2, loss: hash\n",
      "iteration: 420, best: 1, dist: 2, loss: hash\n",
      "iteration: 430, best: 1, dist: 2, loss: hash\n",
      "iteration: 440, best: 1, dist: 2, loss: hash\n",
      "iteration: 450, best: 1, dist: 3, loss: hash\n",
      "iteration: 460, best: 1, dist: 3, loss: hash\n",
      "iteration: 470, best: 1, dist: 3, loss: hash\n",
      "iteration: 480, best: 1, dist: 3, loss: hash\n",
      "iteration: 490, best: 1, dist: 3, loss: hash\n",
      "iteration: 500, best: 1, dist: 3, loss: hash\n",
      "iteration: 510, best: 1, dist: 2, loss: hash\n",
      "iteration: 520, best: 1, dist: 2, loss: hash\n",
      "iteration: 530, best: 1, dist: 2, loss: hash\n",
      "iteration: 540, best: 1, dist: 2, loss: hash\n",
      "iteration: 550, best: 1, dist: 3, loss: hash\n",
      "iteration: 560, best: 1, dist: 3, loss: hash\n",
      "iteration: 570, best: 1, dist: 2, loss: hash\n",
      "iteration: 580, best: 1, dist: 2, loss: hash\n",
      "iteration: 590, best: 1, dist: 2, loss: hash\n",
      "iteration: 600, best: 1, dist: 3, loss: hash\n",
      "iteration: 610, best: 1, dist: 2, loss: hash\n",
      "iteration: 620, best: 1, dist: 2, loss: hash\n",
      "iteration: 630, best: 1, dist: 2, loss: hash\n",
      "iteration: 640, best: 1, dist: 2, loss: hash\n",
      "iteration: 650, best: 1, dist: 2, loss: hash\n",
      "iteration: 660, best: 1, dist: 2, loss: hash\n",
      "iteration: 670, best: 1, dist: 2, loss: hash\n",
      "iteration: 680, best: 1, dist: 2, loss: hash\n",
      "iteration: 690, best: 1, dist: 2, loss: hash\n",
      "iteration: 700, best: 1, dist: 2, loss: hash\n",
      "iteration: 710, best: 1, dist: 2, loss: hash\n",
      "iteration: 720, best: 1, dist: 2, loss: hash\n",
      "iteration: 730, best: 1, dist: 2, loss: hash\n",
      "iteration: 740, best: 1, dist: 2, loss: hash\n",
      "iteration: 750, best: 1, dist: 1, loss: hash\n",
      "iteration: 760, best: 1, dist: 2, loss: hash\n",
      "iteration: 770, best: 1, dist: 2, loss: hash\n",
      "iteration: 780, best: 1, dist: 1, loss: hash\n",
      "iteration: 790, best: 1, dist: 2, loss: hash\n",
      "iteration: 800, best: 1, dist: 2, loss: hash\n",
      "iteration: 810, best: 1, dist: 2, loss: hash\n",
      "iteration: 820, best: 1, dist: 2, loss: hash\n",
      "iteration: 830, best: 1, dist: 2, loss: hash\n",
      "iteration: 840, best: 1, dist: 2, loss: hash\n",
      "iteration: 850, best: 1, dist: 2, loss: hash\n",
      "iteration: 860, best: 1, dist: 2, loss: hash\n",
      "iteration: 870, best: 1, dist: 2, loss: hash\n",
      "iteration: 880, best: 1, dist: 2, loss: hash\n",
      "iteration: 890, best: 1, dist: 2, loss: hash\n",
      "iteration: 900, best: 1, dist: 2, loss: hash\n",
      "iteration: 910, best: 1, dist: 1, loss: hash\n",
      "iteration: 920, best: 1, dist: 2, loss: hash\n",
      "iteration: 930, best: 1, dist: 2, loss: hash\n",
      "iteration: 940, best: 1, dist: 1, loss: hash\n",
      "iteration: 950, best: 0, dist: 0, loss: hash\n",
      "iteration: 960, best: 0, dist: 1, loss: combined\n",
      "iteration: 970, best: 0, dist: 3, loss: hash\n",
      "iteration: 980, best: 0, dist: 3, loss: hash\n",
      "iteration: 990, best: 0, dist: 3, loss: combined\n",
      "iteration: 1000, best: 0, dist: 3, loss: hash\n",
      "iteration: 1010, best: 0, dist: 2, loss: hash\n",
      "iteration: 1020, best: 0, dist: 3, loss: hash\n",
      "iteration: 1030, best: 0, dist: 2, loss: combined\n",
      "iteration: 1040, best: 0, dist: 2, loss: hash\n",
      "iteration: 1050, best: 0, dist: 2, loss: combined\n",
      "iteration: 1060, best: 0, dist: 2, loss: hash\n",
      "iteration: 1070, best: 0, dist: 4, loss: hash\n",
      "iteration: 1080, best: 0, dist: 4, loss: hash\n",
      "iteration: 1090, best: 0, dist: 3, loss: hash\n",
      "iteration: 1100, best: 0, dist: 2, loss: hash\n",
      "iteration: 1110, best: 0, dist: 3, loss: combined\n",
      "iteration: 1120, best: 0, dist: 2, loss: hash\n",
      "iteration: 1130, best: 0, dist: 2, loss: combined\n",
      "iteration: 1140, best: 0, dist: 4, loss: combined\n",
      "iteration: 1150, best: 0, dist: 2, loss: hash\n",
      "iteration: 1160, best: 0, dist: 3, loss: combined\n",
      "iteration: 1170, best: 0, dist: 4, loss: combined\n",
      "iteration: 1180, best: 0, dist: 3, loss: combined\n",
      "iteration: 1190, best: 0, dist: 3, loss: hash\n",
      "iteration: 1200, best: 0, dist: 2, loss: combined\n",
      "iteration: 1210, best: 0, dist: 4, loss: hash\n",
      "iteration: 1220, best: 0, dist: 2, loss: hash\n",
      "iteration: 1230, best: 0, dist: 2, loss: hash\n",
      "iteration: 1240, best: 0, dist: 3, loss: combined\n",
      "iteration: 1250, best: 0, dist: 3, loss: combined\n",
      "iteration: 1260, best: 0, dist: 3, loss: combined\n",
      "iteration: 1270, best: 0, dist: 3, loss: hash\n",
      "iteration: 1280, best: 0, dist: 3, loss: hash\n",
      "iteration: 1290, best: 0, dist: 2, loss: hash\n",
      "iteration: 1300, best: 0, dist: 4, loss: combined\n",
      "iteration: 1310, best: 0, dist: 4, loss: hash\n",
      "iteration: 1320, best: 0, dist: 4, loss: combined\n",
      "iteration: 1330, best: 0, dist: 3, loss: hash\n",
      "iteration: 1340, best: 0, dist: 3, loss: hash\n",
      "iteration: 1350, best: 0, dist: 2, loss: hash\n",
      "iteration: 1360, best: 0, dist: 4, loss: hash\n",
      "iteration: 1370, best: 0, dist: 4, loss: combined\n",
      "iteration: 1380, best: 0, dist: 2, loss: hash\n",
      "iteration: 1390, best: 0, dist: 3, loss: hash\n",
      "iteration: 1400, best: 0, dist: 3, loss: hash\n",
      "iteration: 1410, best: 0, dist: 4, loss: hash\n",
      "iteration: 1420, best: 0, dist: 2, loss: hash\n",
      "iteration: 1430, best: 0, dist: 3, loss: combined\n",
      "iteration: 1440, best: 0, dist: 2, loss: hash\n",
      "iteration: 1450, best: 0, dist: 2, loss: hash\n",
      "iteration: 1460, best: 0, dist: 3, loss: combined\n",
      "iteration: 1470, best: 0, dist: 3, loss: hash\n",
      "iteration: 1480, best: 0, dist: 4, loss: hash\n",
      "iteration: 1490, best: 0, dist: 3, loss: hash\n",
      "iteration: 1500, best: 0, dist: 2, loss: combined\n",
      "iteration: 1510, best: 0, dist: 3, loss: hash\n",
      "iteration: 1520, best: 0, dist: 4, loss: hash\n",
      "iteration: 1530, best: 0, dist: 2, loss: hash\n",
      "iteration: 1540, best: 0, dist: 3, loss: hash\n",
      "iteration: 1550, best: 0, dist: 3, loss: hash\n",
      "iteration: 1560, best: 0, dist: 3, loss: hash\n",
      "iteration: 1570, best: 0, dist: 3, loss: hash\n",
      "iteration: 1580, best: 0, dist: 3, loss: hash\n",
      "iteration: 1590, best: 0, dist: 3, loss: hash\n",
      "iteration: 1600, best: 0, dist: 3, loss: hash\n",
      "iteration: 1610, best: 0, dist: 4, loss: hash\n",
      "iteration: 1620, best: 0, dist: 3, loss: hash\n",
      "iteration: 1630, best: 0, dist: 4, loss: hash\n",
      "iteration: 1640, best: 0, dist: 4, loss: combined\n",
      "iteration: 1650, best: 0, dist: 4, loss: hash\n",
      "iteration: 1660, best: 0, dist: 2, loss: hash\n",
      "iteration: 1670, best: 0, dist: 3, loss: combined\n",
      "iteration: 1680, best: 0, dist: 3, loss: hash\n",
      "iteration: 1690, best: 0, dist: 4, loss: combined\n",
      "iteration: 1700, best: 0, dist: 2, loss: hash\n",
      "iteration: 1710, best: 0, dist: 2, loss: hash\n",
      "iteration: 1720, best: 0, dist: 3, loss: hash\n",
      "iteration: 1730, best: 0, dist: 3, loss: hash\n",
      "iteration: 1740, best: 0, dist: 3, loss: hash\n",
      "iteration: 1750, best: 0, dist: 3, loss: combined\n",
      "iteration: 1760, best: 0, dist: 3, loss: hash\n",
      "iteration: 1770, best: 0, dist: 2, loss: hash\n",
      "iteration: 1780, best: 0, dist: 2, loss: combined\n",
      "iteration: 1790, best: 0, dist: 3, loss: combined\n",
      "iteration: 1800, best: 0, dist: 3, loss: hash\n",
      "iteration: 1810, best: 0, dist: 4, loss: hash\n",
      "iteration: 1820, best: 0, dist: 4, loss: combined\n",
      "iteration: 1830, best: 0, dist: 3, loss: hash\n",
      "iteration: 1840, best: 0, dist: 3, loss: hash\n",
      "iteration: 1850, best: 0, dist: 2, loss: combined\n",
      "iteration: 1860, best: 0, dist: 2, loss: hash\n",
      "iteration: 1870, best: 0, dist: 4, loss: combined\n",
      "iteration: 1880, best: 0, dist: 4, loss: hash\n",
      "iteration: 1890, best: 0, dist: 5, loss: hash\n",
      "iteration: 1900, best: 0, dist: 2, loss: combined\n",
      "iteration: 1910, best: 0, dist: 3, loss: hash\n",
      "iteration: 1920, best: 0, dist: 3, loss: hash\n",
      "iteration: 1930, best: 0, dist: 3, loss: combined\n",
      "iteration: 1940, best: 0, dist: 4, loss: hash\n",
      "iteration: 1950, best: 0, dist: 3, loss: hash\n",
      "iteration: 1960, best: 0, dist: 4, loss: hash\n",
      "iteration: 1970, best: 0, dist: 2, loss: hash\n",
      "iteration: 1980, best: 0, dist: 3, loss: combined\n",
      "iteration: 1990, best: 0, dist: 2, loss: hash\n",
      "iteration: 2000, best: 0, dist: 2, loss: hash\n",
      "iteration: 2010, best: 0, dist: 3, loss: combined\n",
      "iteration: 2020, best: 0, dist: 4, loss: hash\n",
      "iteration: 2030, best: 0, dist: 4, loss: combined\n",
      "iteration: 2040, best: 0, dist: 2, loss: hash\n",
      "iteration: 2050, best: 0, dist: 2, loss: combined\n",
      "iteration: 2060, best: 0, dist: 2, loss: combined\n",
      "iteration: 2070, best: 0, dist: 3, loss: hash\n",
      "iteration: 2080, best: 0, dist: 4, loss: hash\n",
      "iteration: 2090, best: 0, dist: 4, loss: hash\n",
      "iteration: 2100, best: 0, dist: 4, loss: hash\n",
      "iteration: 2110, best: 0, dist: 4, loss: hash\n",
      "iteration: 2120, best: 0, dist: 3, loss: hash\n",
      "iteration: 2130, best: 0, dist: 4, loss: combined\n",
      "iteration: 2140, best: 0, dist: 3, loss: combined\n",
      "iteration: 2150, best: 0, dist: 3, loss: combined\n",
      "iteration: 2160, best: 0, dist: 2, loss: combined\n",
      "iteration: 2170, best: 0, dist: 2, loss: hash\n",
      "iteration: 2180, best: 0, dist: 3, loss: combined\n",
      "iteration: 2190, best: 0, dist: 2, loss: hash\n",
      "iteration: 2200, best: 0, dist: 3, loss: combined\n",
      "iteration: 2210, best: 0, dist: 4, loss: hash\n",
      "iteration: 2220, best: 0, dist: 3, loss: combined\n",
      "iteration: 2230, best: 0, dist: 2, loss: hash\n",
      "iteration: 2240, best: 0, dist: 3, loss: hash\n",
      "iteration: 2250, best: 0, dist: 2, loss: combined\n",
      "iteration: 2260, best: 0, dist: 4, loss: hash\n",
      "iteration: 2270, best: 0, dist: 3, loss: hash\n",
      "iteration: 2280, best: 0, dist: 4, loss: combined\n",
      "iteration: 2290, best: 0, dist: 2, loss: hash\n",
      "iteration: 2300, best: 0, dist: 3, loss: hash\n",
      "iteration: 2310, best: 0, dist: 3, loss: hash\n",
      "iteration: 2320, best: 0, dist: 4, loss: hash\n",
      "iteration: 2330, best: 0, dist: 4, loss: hash\n",
      "iteration: 2340, best: 0, dist: 4, loss: hash\n",
      "iteration: 2350, best: 0, dist: 2, loss: combined\n",
      "iteration: 2360, best: 0, dist: 3, loss: hash\n",
      "iteration: 2370, best: 0, dist: 2, loss: combined\n",
      "iteration: 2380, best: 0, dist: 4, loss: hash\n",
      "iteration: 2390, best: 0, dist: 4, loss: combined\n",
      "iteration: 2400, best: 0, dist: 3, loss: combined\n",
      "iteration: 2410, best: 0, dist: 2, loss: hash\n",
      "iteration: 2420, best: 0, dist: 3, loss: hash\n",
      "iteration: 2430, best: 0, dist: 2, loss: combined\n",
      "iteration: 2440, best: 0, dist: 3, loss: combined\n",
      "iteration: 2450, best: 0, dist: 3, loss: hash\n",
      "iteration: 2460, best: 0, dist: 5, loss: hash\n",
      "iteration: 2470, best: 0, dist: 3, loss: hash\n",
      "iteration: 2480, best: 0, dist: 2, loss: combined\n",
      "iteration: 2490, best: 0, dist: 3, loss: hash\n",
      "iteration: 2500, best: 0, dist: 4, loss: hash\n",
      "iteration: 2510, best: 0, dist: 2, loss: hash\n",
      "iteration: 2520, best: 0, dist: 2, loss: hash\n",
      "iteration: 2530, best: 0, dist: 3, loss: hash\n",
      "iteration: 2540, best: 0, dist: 3, loss: hash\n",
      "iteration: 2550, best: 0, dist: 2, loss: hash\n",
      "iteration: 2560, best: 0, dist: 2, loss: hash\n",
      "iteration: 2570, best: 0, dist: 2, loss: hash\n",
      "iteration: 2580, best: 0, dist: 2, loss: hash\n",
      "iteration: 2590, best: 0, dist: 2, loss: hash\n",
      "iteration: 2600, best: 0, dist: 3, loss: hash\n",
      "iteration: 2610, best: 0, dist: 2, loss: hash\n",
      "iteration: 2620, best: 0, dist: 2, loss: hash\n",
      "iteration: 2630, best: 0, dist: 2, loss: hash\n",
      "iteration: 2640, best: 0, dist: 1, loss: hash\n",
      "iteration: 2650, best: 0, dist: 1, loss: hash\n",
      "iteration: 2660, best: 0, dist: 1, loss: hash\n",
      "iteration: 2670, best: 0, dist: 1, loss: hash\n",
      "iteration: 2680, best: 0, dist: 1, loss: hash\n",
      "iteration: 2690, best: 0, dist: 1, loss: hash\n",
      "iteration: 2700, best: 0, dist: 1, loss: hash\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session(graph=graph) as sess:\n",
    "    for i in range(5000):\n",
    "\n",
    "        \"\"\"\n",
    "        Our attack works as follows:\n",
    "        We initially aim to just alter the image until a collison is found, \n",
    "        that is the new image's bits are the same as h_np. \n",
    "        Once that's achieved choose to minimize the difference \n",
    "        between the new image and the original cat image. \n",
    "        However this is done with the intention that the \n",
    "        changes made upon the image ensure that the difference \n",
    "        in the bits between the hash of the new image and h_np is\n",
    "        at most 2\n",
    "        This ensures that our image doesn't stray too far from \n",
    "        a collision.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        The first if's condition is achieve our current image is a collison,\n",
    "        once that happens the new gradient that we apply the GD method \n",
    "        on is that of the img loss.\n",
    "        This is done so that we start changing our image to minimize \n",
    "        the distance between the current image and the original \n",
    "        cat image.\n",
    "        The reason this is done is because once our image is a collison\n",
    "        all we care about should be to minimize image perturbation\n",
    "        \"\"\"\n",
    "        if dist == 0:\n",
    "            loss_name, loss, g = 'image', img_loss, g_img\n",
    "        \n",
    "        \n",
    "        #The next if condition is achieved when a collision is achieved\n",
    "        #in some iteration in the past (current image need not be collison).\n",
    "        #If that occurs then we want to check whether our current image's\n",
    "        #hash is close to the target, and we define close as being 2 bits off.\n",
    "        #If it is close then we can afford to minimize both hash_loss and \n",
    "        #img_loss without being worried that our has is going to stary away too far\n",
    "        #as it's already close.\n",
    "        \n",
    "        \n",
    "        elif ((best == 0) and (dist <= 2) and (dist > 0)):\n",
    "            loss_name, loss, g = 'combined', combined_loss, g_combined\n",
    "\n",
    "\n",
    "        \n",
    "        #The only alternative left is a collision not having been found, or that \n",
    "        #one has been found but that we've strayed far from the target bits.\n",
    "        #In that case, we ask our algorithm to alter the img in the direction\n",
    "        #whereby hash_loss is minimized to get our image's hash back to being \n",
    "        #close.\n",
    "        \n",
    "        else:\n",
    "            loss_name, loss, g = 'hash', hash_loss, g_hash\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        The next if is made because by iteration 2500 I found that a collision will\n",
    "        have already been found plus the img perturbation will be desirable.\n",
    "        Thus, we just need to ensure that the img stays on track in terms of \n",
    "        being a collision.\n",
    "        Thus, we ask our algo to minimize for hash_loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        if i > 2500:\n",
    "            loss_name, loss, g = 'hash', hash_loss, g_hash\n",
    "            \n",
    "\n",
    "        \"\"\"\n",
    "        As I've mentioned before, for us to run a session on our tf1 graph,\n",
    "        we need to supply the session with the node values for it to complete\n",
    "        a session whereby all output nodes' values can be generated.\n",
    "        Thus, the following dictionary is filled to capture all of that.\n",
    "        \"\"\"\n",
    "\n",
    "        feed = {\n",
    "            image_ph: x,\n",
    "            original_ph: original_np,\n",
    "            seed_ph: seed_np,\n",
    "            h_ph: h_np,     \n",
    "        }\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        The line below is where we run our session on our graph,\n",
    "        moreover in the 1st argument we specify a list of values \n",
    "        and these correspond to all the nodes' values which we want\n",
    "        generated.\n",
    "        \"\"\"\n",
    "        hash_output1, img_loss1, loss1, g1  = sess.run(\n",
    "            [hash_output, img_loss, loss, g],\n",
    "            feed_dict=feed\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        In the next 3 lines we compute the dist value corresponding \n",
    "        to the current img. \n",
    "        This is simply applying <=0.5 and >0.5 thresholding on its\n",
    "        hash_output, that is on the output of the sigmoid function.\n",
    "        \"\"\"\n",
    "\n",
    "        binary_hash = (hash_output1 > 0.5).astype(np.float32)\n",
    "        match = np.sum(binary_hash == h_np)\n",
    "        dist = 96 - match\n",
    "\n",
    "        \"\"\"\n",
    "        Once we have our dist, we can now check if our current\n",
    "        img is a collision.\n",
    "        If this happens to be the case with the number of iterations also being \n",
    "        greater than 2500 then we can break our algorithm due to the \n",
    "        aforementioned reasons.\n",
    "        \"\"\"\n",
    "\n",
    "        if i > 2500 and dist == 0:\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "        if dist < best:\n",
    "\n",
    "            best = dist\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        The gradient we want to use in the GD step is the normalized \n",
    "        version of our generated gradient since I've found that \n",
    "        the gradient values tend to be small in scale (same \n",
    "        reasoning as when we used normalization to generate \n",
    "        the normalized node in the graph construction cell).\n",
    "        \"\"\"\n",
    "\n",
    "        g1 /= (np.linalg.norm(g1)+ 1e-8)\n",
    "        x = x - 0.6 * g1\n",
    "\n",
    "        \"\"\"\n",
    "        We apply this clip step so that our img doesn't stray to \n",
    "        far from the previous img.\n",
    "        \"\"\"\n",
    "        x = np.clip(x, 0.0, 1.0)\n",
    "    \n",
    "        if i%10==0:\n",
    "            \n",
    "            print(\"iteration: {}, best: {}, dist: {}, loss: {}\".format(\n",
    "\n",
    "                i, best, dist, loss_name\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f8900f73-b569-49ca-9056-ea99496b0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(x, os.path.join('.', \"cat_attack.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
